{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gzip\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle \n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from itertools import permutations\n",
    "from multiprocessing import Pool, TimeoutError\n",
    "\n",
    "class PositionOptimizer:\n",
    "    def __init__(self, clfs_path: str, hero_path: str) -> None:\n",
    "        self.clfs_path = clfs_path\n",
    "        self.hero_path = hero_path\n",
    "        self.opendota_data = {}\n",
    "        self.clfs = defaultdict(list)\n",
    "        self.role_counts = defaultdict(dict)\n",
    "        self.hero_data = {}\n",
    "\n",
    "        self._load_clfs(clfs_path)\n",
    "        self._load_hero_data(hero_path)\n",
    "        self._load_opendota_data()\n",
    "        self.hid_to_name = {h[\"id\"]: h[\"localized_name\"] for h in self.hero_data}\n",
    "\n",
    "    def find_optimal_roles(self, match):\n",
    "        players = match[\"players\"]\n",
    "        t0 = [p for p in players if p[\"player_slot\"] < 128]\n",
    "        t1 = [p for p in players if p[\"player_slot\"] >= 128]\n",
    "        team_optimal_positions = {}\n",
    "\n",
    "        for team, marker in zip([t0, t1], [0, 1]):\n",
    "            # !Ranks\n",
    "            attributes = [\"gold_per_min\", \"xp_per_min\", \"kills\", \"deaths\", \"assists\", \"last_hits\", \"hero_damage\", \"tower_damage\"]\n",
    "            ranks = {attr: sorted([(p[\"hero_id\"], p[attr]) for p in team], key=lambda x: x[1], reverse=True) for attr in attributes}\n",
    "            hids = [p[\"hero_id\"] for p in team]\n",
    "            team_position_proba = defaultdict(list)\n",
    "\n",
    "            # !Create c\n",
    "            for p in team:\n",
    "                features = []\n",
    "                hid = p[\"hero_id\"]\n",
    "\n",
    "                teammates = np.zeros((136))\n",
    "                for team_hid in hids:\n",
    "                    if team_hid != hid:\n",
    "                        teammates[team_hid] = 1.\n",
    "                \n",
    "                features.append(teammates)\n",
    "                \n",
    "                for rank in ranks: \n",
    "                    r = ranks[rank].index((hid, p[rank]))\n",
    "                    features.append(self._get_rank(r))\n",
    "\n",
    "                x = np.concatenate(features)\n",
    "                y_pred = self.clfs[hid].predict_log_proba(x.reshape(1, -1))\n",
    "                # updated_y_pred = self._remove_unplayed_roles(hid, y_pred.ravel())         \n",
    "                team_position_proba[hid] = y_pred.ravel()\n",
    "            best_log_p = -np.inf\n",
    "            best_comp = None\n",
    "\n",
    "            # Optimal \n",
    "            for comp in permutations(range(5), 5):\n",
    "                # comp_with_heroid = [(comp[i], hid, self.hid_to_name[hid], round(team_position_proba[hid][comp[i]], 2)) for i, hid in enumerate(hids)]\n",
    "                comp_with_heroid = [(comp[i], hid, self.hid_to_name[hid], team_position_proba[hid][comp[i]]) for i, hid in enumerate(hids)]\n",
    "                \n",
    "                comp_with_heroid_dict = {c[1]: (c[0], c[2], c[3]) for c in comp_with_heroid}\n",
    "                \n",
    "                log_p = np.array([team_position_proba[hid][comp[i]] for i, hid in enumerate(hids)]).sum()\n",
    "                if log_p > best_log_p:\n",
    "                    best_log_p = log_p\n",
    "                    best_comp = comp_with_heroid_dict\n",
    "                    # best_comp = sorted(comp_with_heroid, key=lambda x: x[0])\n",
    "            \n",
    "            team_optimal_positions[marker] = best_comp\n",
    "            # print(\"{} => {:.2f}\".format(best_comp, best_log_p))\n",
    "\n",
    "        # Return both teams\n",
    "        return team_optimal_positions\n",
    "\n",
    "    def _remove_unplayed_roles(self, hid, y_pred, threshold=200):\n",
    "        ys = self.opendota_data[\"ys\"]\n",
    "        role_counts = dict(Counter(ys[hid]))\n",
    "        updated_y_pred = np.zeros(y_pred.shape)\n",
    "\n",
    "        for k in range(0,5):\n",
    "            if role_counts[k + 1] < threshold:\n",
    "                updated_y_pred[k] = -1000.\n",
    "            else:\n",
    "                updated_y_pred[k] = y_pred[k]\n",
    "        return updated_y_pred\n",
    "\n",
    "    def _get_rank(self, rank):\n",
    "        oh = np.zeros(5)\n",
    "        oh[rank] = 1\n",
    "        return oh\n",
    "\n",
    "    def _load_clfs(self, clf_path):\n",
    "        \"\"\"\n",
    "        Load clfs from pickle file\n",
    "        \"\"\"\n",
    "        with open(clf_path, 'rb') as f:\n",
    "            self.clfs = pickle.load(f)\n",
    "\n",
    "    def _load_opendota_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Load opendota data from data/opendota_data.json\n",
    "        \"\"\"\n",
    "        with open('../position_optimizer/data/dataset_positions_all.pkl', 'rb') as f:\n",
    "            self.opendota_data = pickle.load(f)\n",
    "\n",
    "\n",
    "    def _load_hero_data(self, hero_path) -> None:\n",
    "        \"\"\"\n",
    "        Load hero data from data/heroes.json\n",
    "        \"\"\"\n",
    "        with open(hero_path, 'r') as f:\n",
    "            self.hero_data = json.load(f)\n",
    "\n",
    "    def get_all_hids(self):\n",
    "        return [h[\"id\"] for h in self.hero_data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotate each hero with position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bao/anaconda3/envs/dota/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.24.2 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to:  ../data/virtual_loss_training_data/example_10000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_20000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_30000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_40000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_50000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_60000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_70000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_80000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_90000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_100000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_110000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_120000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_130000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_140000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_150000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_160000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_170000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_180000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_190000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_200000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_210000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_220000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_230000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_240000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_250000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_260000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_270000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_280000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_290000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_300000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_310000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_320000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_330000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_340000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_350000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_360000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_370000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_380000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_390000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_400000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_410000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_420000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_430000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_440000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_450000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_460000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_470000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_480000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_490000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_500000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_510000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_520000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_530000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_540000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_550000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_560000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_570000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_580000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_590000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_600000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_610000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_620000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_630000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_640000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_650000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_660000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_670000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_680000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_690000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_700000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_710000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_720000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_730000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_740000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_750000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_760000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_770000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_780000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_790000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_800000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_810000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_820000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_830000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_840000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_850000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_860000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_870000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_880000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_890000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_900000.pkl\n",
      "saved to:  ../data/virtual_loss_training_data/example_905547.pkl\n"
     ]
    }
   ],
   "source": [
    "def is_valid_match(match, all_hids):\n",
    "    if \"match_id\" not in match:\n",
    "        return False\n",
    "    if not match[\"lobby_type\"] == 7:\n",
    "        # print(\"Invalid lobby type:\", match[\"lobby_type\"])\n",
    "        return False\n",
    "    if match[\"duration\"] < 60 * 20 and match[\"duration\"] > 60 * 55:\n",
    "        # print(\"Invalid duration:\", match[\"duration\"])\n",
    "        return False\n",
    "    if not match[\"game_mode\"] in {1, 2, 16, 22}:\n",
    "        # print(\"Invalid game mode:\", match[\"game_mode\"])\n",
    "        return False\n",
    "\n",
    "    for p in match[\"players\"]:\n",
    "        if p[\"hero_id\"] not in all_hids:\n",
    "            # print(\"Invalid hero id:\", p[\"hero_id\"])\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def process_matches(lines, n_examples, po, all_hids):\n",
    "    out_path = \"../data/virtual_loss_training_data/example_{}.pkl\".format(n_examples)\n",
    "    training_data = []\n",
    "    winners = []\n",
    "\n",
    "    # for line in tqdm.tqdm(fp):\n",
    "    match_array = np.zeros(shape=(121, 5))\n",
    "    for line in lines:\n",
    "        match_array.fill(0)\n",
    "        match = json.loads(line)       \n",
    "         \n",
    "        # Check if match is valid\n",
    "        if not is_valid_match(match, all_hids):\n",
    "            continue\n",
    "        \n",
    "        players = match[\"players\"]\n",
    "        t0 = [p for p in players if p[\"player_slot\"] < 128]\n",
    "        t1 = [p for p in players if p[\"player_slot\"] >= 128]\n",
    "\n",
    "        optimal_positions = po.find_optimal_roles(match)\n",
    "        for i, h in enumerate(po.hero_data):\n",
    "            participant_hero_lane = np.zeros(shape=(5))\n",
    "            for p in t0:\n",
    "                hid = p['hero_id']\n",
    "                pos = optimal_positions[0][hid][0]\n",
    "                if hid == h['id']:\n",
    "                    participant_hero_lane[pos] = 1\n",
    "                    match_array[i] = participant_hero_lane\n",
    "            for p in t1:\n",
    "                hid = p['hero_id']\n",
    "                pos = optimal_positions[1][hid][0]\n",
    "                if hid == h['id']:\n",
    "                    participant_hero_lane[pos] = -1\n",
    "                    match_array[i] = participant_hero_lane\n",
    "\n",
    "        training_data.append(np.concatenate(match_array))\n",
    "        winners.append(match[\"radiant_win\"])\n",
    "            \n",
    "        \n",
    "    x = np.vstack(training_data).astype(np.float32)\n",
    "    y = np.array(winners)\n",
    "\n",
    "    dataset = {\n",
    "        \"x\": x,\n",
    "        \"y\": y\n",
    "    }\n",
    "    \n",
    "    with open(out_path, \"wb\") as f:\n",
    "        pickle.dump(dataset, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(\"saved to: \", out_path)\n",
    "\n",
    "buffer_size = 10000\n",
    "n_examples = 0\n",
    "po = PositionOptimizer('../data/clfs/logreg_clfs_all.pkl', '../data/heroes.json')\n",
    "all_hids = po.get_all_hids()\n",
    "\n",
    "with gzip.open(\"../data/raw/dataset_batch1_900k.gz\", \"r\") as fp:\n",
    "    buffer = []\n",
    "    for line in fp:\n",
    "        n_examples += 1\n",
    "        buffer.append(line)\n",
    "        if len(buffer) == buffer_size:\n",
    "            process_matches(buffer, n_examples, po, all_hids)\n",
    "            buffer = []\n",
    "\n",
    "    # process remaining\n",
    "    if len(buffer) > 0:\n",
    "        process_matches(buffer, n_examples, po, all_hids)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7463, 605) (7463,)\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/virtual_loss_training_data/example_10000.pkl\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "    x = dataset[\"x\"]\n",
    "    y = dataset[\"y\"]\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76d279d9dda3f104956abaa709c4930fec7a5837133fea297357827d52b43a61"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('dota': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
