{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c0e9d1-1ac1-4df8-978a-89e91a579f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gzip\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle \n",
    "import tarfile\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from itertools import permutations\n",
    "from multiprocessing import Pool, TimeoutError\n",
    "\n",
    "class PositionOptimizer:\n",
    "    def __init__(self, clfs_path: str, hero_path: str) -> None:\n",
    "        self.clfs_path = clfs_path\n",
    "        self.hero_path = hero_path\n",
    "        self.opendota_data = {}\n",
    "        self.clfs = defaultdict(list)\n",
    "        self.role_counts = defaultdict(dict)\n",
    "        self.hero_data = {}\n",
    "\n",
    "        self._load_clfs(clfs_path)\n",
    "        self._load_hero_data(hero_path)\n",
    "        self._load_opendota_data()\n",
    "        self.hid_to_name = {h[\"id\"]: h[\"localized_name\"] for h in self.hero_data}\n",
    "\n",
    "    def find_optimal_roles(self, match):\n",
    "        players = match[\"players\"]\n",
    "        t0 = [p for p in players if p[\"player_slot\"] < 128]\n",
    "        t1 = [p for p in players if p[\"player_slot\"] >= 128]\n",
    "        team_optimal_positions = {}\n",
    "\n",
    "        for team, marker in zip([t0, t1], [0, 1]):\n",
    "            # !Ranks\n",
    "            attributes = [\"gold_per_min\", \"xp_per_min\", \"kills\", \"deaths\", \"assists\", \"last_hits\", \"hero_damage\", \"tower_damage\"]\n",
    "            ranks = {attr: sorted([(p[\"hero_id\"], p[attr]) for p in team], key=lambda x: x[1], reverse=True) for attr in attributes}\n",
    "            hids = [p[\"hero_id\"] for p in team]\n",
    "            team_position_proba = defaultdict(list)\n",
    "\n",
    "            # !Create c\n",
    "            for p in team:\n",
    "                features = []\n",
    "                hid = p[\"hero_id\"]\n",
    "\n",
    "                teammates = np.zeros((136))\n",
    "                for team_hid in hids:\n",
    "                    if team_hid != hid:\n",
    "                        teammates[team_hid] = 1.\n",
    "                \n",
    "                features.append(teammates)\n",
    "                \n",
    "                for rank in ranks: \n",
    "                    r = ranks[rank].index((hid, p[rank]))\n",
    "                    features.append(self._get_rank(r))\n",
    "\n",
    "                x = np.concatenate(features)\n",
    "                y_pred = self.clfs[hid].predict_log_proba(x.reshape(1, -1))\n",
    "                # updated_y_pred = self._remove_unplayed_roles(hid, y_pred.ravel())         \n",
    "                team_position_proba[hid] = y_pred.ravel()\n",
    "            best_log_p = -np.inf\n",
    "            best_comp = None\n",
    "\n",
    "            # Optimal \n",
    "            for comp in permutations(range(5), 5):\n",
    "                # comp_with_heroid = [(comp[i], hid, self.hid_to_name[hid], round(team_position_proba[hid][comp[i]], 2)) for i, hid in enumerate(hids)]\n",
    "                comp_with_heroid = [(comp[i], hid, self.hid_to_name[hid], team_position_proba[hid][comp[i]]) for i, hid in enumerate(hids)]\n",
    "                \n",
    "                comp_with_heroid_dict = {c[1]: (c[0], c[2], c[3]) for c in comp_with_heroid}\n",
    "                \n",
    "                log_p = np.array([team_position_proba[hid][comp[i]] for i, hid in enumerate(hids)]).sum()\n",
    "                if log_p > best_log_p:\n",
    "                    best_log_p = log_p\n",
    "                    best_comp = comp_with_heroid_dict\n",
    "                    # best_comp = sorted(comp_with_heroid, key=lambda x: x[0])\n",
    "            \n",
    "            team_optimal_positions[marker] = best_comp\n",
    "            # print(\"{} => {:.2f}\".format(best_comp, best_log_p))\n",
    "\n",
    "        # Return both teams\n",
    "        return team_optimal_positions\n",
    "\n",
    "    def _remove_unplayed_roles(self, hid, y_pred, threshold=200):\n",
    "        ys = self.opendota_data[\"ys\"]\n",
    "        role_counts = dict(Counter(ys[hid]))\n",
    "        updated_y_pred = np.zeros(y_pred.shape)\n",
    "\n",
    "        for k in range(0,5):\n",
    "            if role_counts[k + 1] < threshold:\n",
    "                updated_y_pred[k] = -1000.\n",
    "            else:\n",
    "                updated_y_pred[k] = y_pred[k]\n",
    "        return updated_y_pred\n",
    "\n",
    "    def _get_rank(self, rank):\n",
    "        oh = np.zeros(5)\n",
    "        oh[rank] = 1\n",
    "        return oh\n",
    "\n",
    "    def _load_clfs(self, clf_path):\n",
    "        \"\"\"\n",
    "        Load clfs from pickle file\n",
    "        \"\"\"\n",
    "        with open(clf_path, 'rb') as f:\n",
    "            self.clfs = pickle.load(f)\n",
    "\n",
    "    def _load_opendota_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Load opendota data from data/opendota_data.json\n",
    "        \"\"\"\n",
    "        with open('data/dataset_positions_all.pkl', 'rb') as f:\n",
    "            self.opendota_data = pickle.load(f)\n",
    "\n",
    "\n",
    "    def _load_hero_data(self, hero_path) -> None:\n",
    "        \"\"\"\n",
    "        Load hero data from data/heroes.json\n",
    "        \"\"\"\n",
    "        with open(hero_path, 'r') as f:\n",
    "            self.hero_data = json.load(f)\n",
    "\n",
    "    def get_all_hids(self):\n",
    "        return [h[\"id\"] for h in self.hero_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff56cb6-c5d3-4449-9c1b-28c65792f037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_match(match, all_hids):\n",
    "    if \"match_id\" not in match:\n",
    "        return False\n",
    "    if not match[\"lobby_type\"] == 7:\n",
    "        # print(\"Invalid lobby type:\", match[\"lobby_type\"])\n",
    "        return False\n",
    "    if match[\"duration\"] < 60 * 20 and match[\"duration\"] > 60 * 55:\n",
    "        # print(\"Invalid duration:\", match[\"duration\"])\n",
    "        return False\n",
    "    if not match[\"game_mode\"] in {1, 2, 16, 22}:\n",
    "        # print(\"Invalid game mode:\", match[\"game_mode\"])\n",
    "        return False\n",
    "\n",
    "    for p in match[\"players\"]:\n",
    "        if p[\"hero_id\"] not in all_hids:\n",
    "            # print(\"Invalid hero id:\", p[\"hero_id\"])\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def process_matches(lines, n_examples, po, all_hids):\n",
    "    out_path = \"data/virtual_loss_training_data/example_{}.pkl\".format(n_examples)\n",
    "    training_data = []\n",
    "    winners = []\n",
    "\n",
    "    match_array = np.zeros(shape=(121, 5))\n",
    "    for line in lines:\n",
    "        match_array.fill(0)\n",
    "        match = json.loads(line)       \n",
    "         \n",
    "        # Check if match is valid\n",
    "        if not is_valid_match(match, all_hids):\n",
    "            continue\n",
    "        \n",
    "        players = match[\"players\"]\n",
    "        t0 = [p for p in players if p[\"player_slot\"] < 128]\n",
    "        t1 = [p for p in players if p[\"player_slot\"] >= 128]\n",
    "\n",
    "        optimal_positions = po.find_optimal_roles(match)\n",
    "        for i, h in enumerate(po.hero_data):\n",
    "            participant_hero_lane = np.zeros(shape=(5))\n",
    "            for p in t0:\n",
    "                hid = p['hero_id']\n",
    "                pos = optimal_positions[0][hid][0]\n",
    "                if hid == h['id']:\n",
    "                    participant_hero_lane[pos] = 1\n",
    "                    match_array[i] = participant_hero_lane\n",
    "            for p in t1:\n",
    "                hid = p['hero_id']\n",
    "                pos = optimal_positions[1][hid][0]\n",
    "                if hid == h['id']:\n",
    "                    participant_hero_lane[pos] = -1\n",
    "                    match_array[i] = participant_hero_lane\n",
    "\n",
    "        training_data.append(np.concatenate(match_array))\n",
    "        winners.append(match[\"radiant_win\"])\n",
    "            \n",
    "        \n",
    "    x = np.vstack(training_data).astype(np.float32)\n",
    "    y = np.array(winners)\n",
    "\n",
    "    dataset = {\n",
    "        \"x\": x,\n",
    "        \"y\": y\n",
    "    }\n",
    "    \n",
    "    with open(out_path, \"wb\") as f:\n",
    "        pickle.dump(dataset, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(\"saved to: \", out_path)\n",
    "\n",
    "    \n",
    "# FOR THE SMALL DATASET\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     buffer_size = 10000\n",
    "#     n_examples = 0\n",
    "#     po = PositionOptimizer('data/clfs/logreg_clfs_all.pkl', 'data/heroes.json')\n",
    "#     all_hids = po.get_all_hids()\n",
    "#     with Pool(processes=os.cpu_count()-4) as pool:\n",
    "#         with gzip.open(\"data/dataset_batch1_900k.gz\", \"r\") as fp:\n",
    "#             buffer = []\n",
    "#             for line in fp:\n",
    "#                 n_examples += 1\n",
    "#                 buffer.append(line)\n",
    "#                 if len(buffer) == buffer_size:\n",
    "#                     kwds = {\n",
    "#                         \"lines\": buffer,\n",
    "#                         \"n_examples\": n_examples,\n",
    "#                         \"po\": po,\n",
    "#                         \"all_hids\": all_hids\n",
    "#                     }\n",
    "#                     pool.apply_async(process_matches, kwds=kwds)\n",
    "#                     buffer = []\n",
    "\n",
    "#             # process remaining\n",
    "#             if len(buffer) > 0:\n",
    "#                 process_matches(buffer, n_examples, po, all_hids)\n",
    "\n",
    "\n",
    "# FOR THE BIG\n",
    "if __name__ == \"__main__\":\n",
    "    buffer_size = 100000\n",
    "    n_examples = 0\n",
    "    po = PositionOptimizer('data/clfs/logreg_clfs_all.pkl', 'data/heroes.json')\n",
    "    all_hids = po.get_all_hids()\n",
    "    with Pool(processes=os.cpu_count()-4) as pool:\n",
    "        with tarfile.open(\"data/dota2_matches_30669739_samples.tar.gz\", 'r:gz') as tar:       \n",
    "            buffer = []\n",
    "            for batch_info in tar:\n",
    "                for line in tar.extractfile(batch_info):                    \n",
    "                    n_examples += 1\n",
    "                    buffer.append(line)\n",
    "                    if len(buffer) == buffer_size:\n",
    "                        kwds = {\n",
    "                            \"lines\": buffer,\n",
    "                            \"n_examples\": n_examples,\n",
    "                            \"po\": po,\n",
    "                            \"all_hids\": all_hids\n",
    "                        }\n",
    "                        pool.apply_async(process_matches, kwds=kwds)\n",
    "                        buffer = []\n",
    "\n",
    "                # process remaining\n",
    "                if len(buffer) > 0:\n",
    "                    process_matches(buffer, n_examples, po, all_hids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b18222-4007-4397-a24b-3c3bb98fe5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tarfile.open(\"data/dota2_matches_30669739_samples.tar.gz\", 'r:gz') as tar:       \n",
    "#     for batch_info in tar:\n",
    "#         for line in tar.extractfile(batch_info):\n",
    "#             print(json.loads(line))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4545a38e-f76f-487d-b46e-07172e11e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the xs sparse\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "out_folder = \"data/vl_train_data_sparse\"\n",
    "\n",
    "for file in glob.glob(\"data/virtual_loss_training_data/*.pkl\"):\n",
    "    fname = os.path.basename(file)\n",
    "    out_path = os.path.join(out_folder, fname)\n",
    "    \n",
    "    with open(file, 'rb') as fp:\n",
    "        data = pickle.load(fp)\n",
    "    \n",
    "    x = data[\"x\"]\n",
    "    y = data[\"y\"]\n",
    "\n",
    "    x_sp = csr_matrix(x)\n",
    "\n",
    "    new_out = {\"x\": x_sp, \"y\": y}\n",
    "\n",
    "    # save new_out as pkl\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        pickle.dump(new_out, f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1289361-c240-4f08-9452-ed232520a3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
